---
title: "Getting Started: R and BigQuery"
format: 
  revealjs:
    theme: [serif, custom.scss]
    scrollable: true
    footer: Joy Payton, Children's Hospital of Philadelphia
    css: styles.css
editor: visual
---

------------------------------------------------------------------------

This slide deck was built in Quarto!

-   Use keyboard arrow keys to
    -   advance ( → ) and
    -   go back ( ← )
-   Type "s" to see speaker notes
-   Type "?" to see other keyboard shortcuts

```{r echo = FALSE}
library(countdown)
```

# Progress So Far {.smaller}

So far, you've:

* Signed up for GCP
* Created a project
* Added BigQuery to that project
* Looked at public data in another project
* Used that data to create a query and a table in your project

# What's Left?

* Exporting your table for use elsewhere
* Connecting directly to BigQuery from RStudio
* (Maybe) Vertex AI Workbench

# Let's Export! {.smaller}

![](media/export_options.png){.bordered}
::: notes
One way you can work with the data that you've groomed and selected from BigQuery is by exporting it to use elsewhere.  Depending on the size of the data and how often the data changes, this might be a great use case, and very convenient.  Let's go over that briefly.

Now, you might think that the Export button as you're looking at a table would be a great way to get data out of GCP, but this export button is actually for exporting to other Google products, like Looker Studio or Google Cloud Storage.  If that's what you want, fantastic, but let's assume you want to take a .csv and just run with that data somewhere else.  What should you do?

To export a .csv, you need to have query results, and you can save those in various formats.  So we have a couple of options.
:::

# Getting Query Results to Save {.smaller}

Since you've saved the query you used to create this table, you can re-run that query.

Or, since you've created a table that contains all the data you want, you run a "give me everything" query:

```
SELECT * FROM [table_name]
```

# Saving Data Exercise

Run a query -- either your saved query or a "SELECT * FROM..." on your saved table.

Then Click the Save Results button and save the .csv to your computer.


```{r echo = FALSE}
library(countdown)
countdown(minutes = 3, seconds = 00)
```


# R vs RStudio {.smaller}

Quick reminder: R is a language, that can be run in many settings:

-   from the command line
-   in an automation (like a cron job)
-   from the R IDE (very bare bones)
-   from the RStudio IDE
-   within a notebook like a Jupyter notebook

RStudio is a fully featured IDE that runs on Linux, Windows, and Mac.  It's much more heavy weight than just the language.

# Working with RStudio IDE {.smaller}

Many of us love RStudio. You can spin up an RStudio server in GCP... but you don't have to, and it might mean spending money when you don't need to.

You can use your **normal** RStudio (on your computer or in Posit.cloud) to work with BigQuery.

# Following Along

Visit <https://github.com/pm0kjp/rmedicine_2024_bigquery/blob/main/bq_demo.qmd> and get the code, but we'll also do this together bit by bit.

# Using non-GCP RStudio {.smaller}

Please open whatever RStudio you typically use. <https://posit.cloud> is a good option!

When you get there, open a new Quarto or R Markdown document.

```{r echo = FALSE}
library(countdown)
countdown(minutes = 3, seconds = 00)
```

::: notes
OK, so we'll be working now in RStudio. If you use RStudio from your local computer, you can open that. Or, if you prefer working in Posit.cloud, you can open that. That's what I'll be doing today. In fact, it might be useful to open a Posit.cloud account if you haven't already. Just go to posit.cloud and hit sign up. I'll give everyone a couple of minutes to fire up their RStudio.
:::

# bigrquery

There are several ways to connect to BigQuery (see, e.g. <https://solutions.posit.co/connections/db/databases/big-query/>) but the way I'm going to show you uses a library called `bigrquery`. 

Please install `bigrquery` as well as `tidyverse`, if you don't already have that installed.

Then you're going to start a new Quarto or R Markdown document and add a code chunk that loads these two libraries:

![](media/libraries.png){.one-half .bordered}

::: notes

There are several ways to connect to BigQuery (see, e.g. <https://solutions.posit.co/connections/db/databases/big-query/>) but the way I'm going to show you uses a library called `bigrquery`.  I'd like you to use `install.packages` to install `bigrquery` as well as `tidyverse`, if you don't already have that installed.

Then you're going to start a new Quarto or R Markdown document and add a code chunk that loads these two libraries.  I'm going to hop into screen share and go ahead and install these packages into a new Posit.cloud Quarto document.

I'll log in to Posit.cloud, and create a new project.  Once that is created, I'll make a new Quarto document, and change the view to source instead of visual.

I'll get rid of this boilerplate and add in the installs and library calls and run those cells.  This may take a while!  While that's installing, let me go to authentication, and return to my slides.

:::

# Authenticating {.smaller}

Now you need to authenticate so that RStudio can connect to BigQuery.  You'll next type:

```
bq_auth()
```

Use the same Google Identity you're using for GCP.  If it suggests installing `httpuv`, do it, as otherwise authorization might fail.

::: notes

Now you need to authenticate so that RStudio can connect to BigQuery.  You'll next type bq_auth() and follow the directions.  It's important to make sure that you do the authorization using the same Google Identity you're using for GCP.  You'll be asked whether you want to store your credentials to make it easier next time, and I usually say yes.  Then, a URL will open for you, or you might be given a URL to open.  Make sure you are using the identity that you're using for GCP!  You'll be prompted to give permission to Tidyverse API, which is safe, and in the last step you'll be given a code to enter into your R console.

I'll show you what this looks like in the slides here, and just run through them very quickly, then show you what it looks like in a live coding screenshare.
:::

## Step 1: Store (or not) Credential

![](media/store_credentials.png){.bordered}

## Step 2: Sign in With Correct Identity

![](media/choose_account.png){.bordered}

## Step 3: Give permissions

![](media/sign_in_tidyverse.png){.bordered}![](media/grant_access.png){.bordered}

## Step 4: Get Authorization Code {.smaller}

You might or might not see this -- you might get this code passed back automatically with a message that says 

```
Authentication complete. Please close this page and return to R.
```

![](media/complete_auth_process.png){.bordered}

## Step 5: Enter Auth Code

![](media/enter_authorization_code.png){.bordered}
::: notes

OK, so now you've seen what this looks like, I'm going to do a screen share, but I'm going to stop sharing before it shares my authentication code, so you'll just have to imagine those last steps.
:::

# Run a Query from RStudio {.smaller}

The pattern is: 

* Set your `project_id` and `my_sql_query` objects
* Send that query to BigQuery using `results <- bq_project_query(project_id, my_sql_query)`
* Get the table via `df <- bq_table_download(results)`

# Take a look!  {.smaller}

Visit <https://github.com/pm0kjp/rmedicine_2024_bigquery/blob/main/bq_demo.qmd> and get the code, but we'll also do this together bit by bit.

::: notes

Okay, so let me walk you through what this will be like.  I'm going to make a new code chunk, and create two new objects, one of which will hold my project id, which I can get from my GCP project dashboard, and one of which will hold my SQL query.  Let's do that together.

Then, once I have my query and my project ID set, I'm ready to use bigrquery commands to run that query, and then to retrieve the data.  Let's add that code to a new code chunk, and run it.

And now, in RStudio, I have this data in my environment.  Let's take a look at that data by using View.
:::




# Vertex AI Workbench

![](media/vertex_ai_workbench.png){.bordered}![](media/enable_notebooks_api.png){.bordered}
::: note
In the burger menu, select Vertex AI, then Workbench.  Enable the api.

:::


# Create a Workbench Instance

![](media/create_new_instance.png){.bordered}

Important to shut down: <https://cloud.google.com/vertex-ai/docs/workbench/instances/shut-down>

::: notes
You'll need to create a workbench instance.  This is going to use compute resources that cost money, so it's important to remember to shut down this instance when you're not using it. <https://cloud.google.com/vertex-ai/docs/workbench/instances/shut-down>.  Default settings include a 3 hour idle timer, which is useful!  Now, I'm going to be very honest with you and disclose that I never use Jupyter for working with R, so this might be a bit cludgy.  I've cobbled together some instructions but I'm not a pro, so we might end up doing some googling here!
:::

# Setting up R kernel {.smaller}

In a new Terminal in your Jupyter instance, enter the following:
```
conda create -n r
conda config --add channels conda-forge
conda install -c conda-forge r-base
conda install -c conda-forge r-essentials
conda install -c conda-forge r-tidyverse
conda install -c conda-forge r-stringr
conda install -c conda-forge r-gargle
conda install -c conda-forge r-bigrquery
conda activate r
```
Answer "y" when prompted.

Once all this is installed, issue this command:
```
conda activate r
```
Answer "y" when prompted.

::: notes

:::

# Start Notebook with R kernel

File > New > Notebook

Select "R" as the kernel

